"""
This type stub file was generated by pyright.
"""

import marshal
import re
import tempfile
import threading
import time
from hashlib import md5
from math import log
from . import finalseg
from ._compat import *
from shutil import move as _replace_file

__version__ = ...
__license__ = ...
if os.name == 'nt':
    ...
else:
    ...
_get_abs_path = ...
DEFAULT_DICT = ...
DEFAULT_DICT_NAME = ...
log_console = ...
default_logger = ...
DICT_WRITING = ...
pool = ...
re_userdict = ...
re_eng = ...
re_han_default = ...
re_skip_default = ...
def setLogLevel(log_level): # -> None:
    ...

class Tokenizer:
    def __init__(self, dictionary=...) -> None:
        ...
    
    def __repr__(self): # -> str:
        ...
    
    @staticmethod
    def gen_pfdict(f): # -> tuple[dict[Any, Any], int]:
        ...
    
    def initialize(self, dictionary=...): # -> None:
        ...
    
    def check_initialized(self): # -> None:
        ...
    
    def calc(self, sentence, DAG, route): # -> None:
        ...
    
    def get_DAG(self, sentence): # -> dict[Any, Any]:
        ...
    
    def cut(self, sentence, cut_all=..., HMM=..., use_paddle=...): # -> Generator[Any | str, Any, None]:
        """
        The main function that segments an entire sentence that contains
        Chinese characters into separated words.

        Parameter:
            - sentence: The str(unicode) to be segmented.
            - cut_all: Model type. True for full pattern, False for accurate pattern.
            - HMM: Whether to use the Hidden Markov Model.
        """
        ...
    
    def cut_for_search(self, sentence, HMM=...): # -> Generator[Any | str, Any, None]:
        """
        Finer segmentation for search engines.
        """
        ...
    
    def lcut(self, *args, **kwargs): # -> list[Any | str]:
        ...
    
    def lcut_for_search(self, *args, **kwargs): # -> list[Any | str]:
        ...
    
    _lcut = ...
    _lcut_for_search = ...
    def get_dict_file(self): # -> _ResourceStream:
        ...
    
    def load_userdict(self, f): # -> None:
        '''
        Load personalized dict to improve detect rate.

        Parameter:
            - f : A plain text file contains words and their ocurrences.
                  Can be a file-like object, or the path of the dictionary file,
                  whose encoding must be utf-8.

        Structure of dict file:
        word1 freq1 word_type1
        word2 freq2 word_type2
        ...
        Word type may be ignored
        '''
        ...
    
    def add_word(self, word, freq=..., tag=...): # -> None:
        """
        Add a word to dictionary.

        freq and tag can be omitted, freq defaults to be a calculated value
        that ensures the word can be cut out.
        """
        ...
    
    def del_word(self, word): # -> None:
        """
        Convenient function for deleting a word.
        """
        ...
    
    def suggest_freq(self, segment, tune=...): # -> int:
        """
        Suggest word frequency to force the characters in a word to be
        joined or splitted.

        Parameter:
            - segment : The segments that the word is expected to be cut into,
                        If the word should be treated as a whole, use a str.
            - tune : If True, tune the word frequency.

        Note that HMM may affect the final result. If the result doesn't change,
        set HMM=False.
        """
        ...
    
    def tokenize(self, unicode_sentence, mode=..., HMM=...): # -> Generator[tuple[Any | str, int, int], Any, None]:
        """
        Tokenize a sentence and yields tuples of (word, start, end)

        Parameter:
            - sentence: the str(unicode) to be segmented.
            - mode: "default" or "search", "search" is for finer segmentation.
            - HMM: whether to use the Hidden Markov Model.
        """
        ...
    
    def set_dictionary(self, dictionary_path): # -> None:
        ...
    


dt = ...
get_FREQ = ...
add_word = ...
calc = ...
cut = ...
lcut = ...
cut_for_search = ...
lcut_for_search = ...
del_word = ...
get_DAG = ...
get_dict_file = ...
initialize = ...
load_userdict = ...
set_dictionary = ...
suggest_freq = ...
tokenize = ...
user_word_tag_tab = ...
def enable_parallel(processnum=...):
    """
    Change the module's `cut` and `cut_for_search` functions to the
    parallel version.

    Note that this only works using dt, custom Tokenizer
    instances are not supported.
    """
    ...

def disable_parallel(): # -> None:
    ...

